{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Heart Disease\n",
    "\n",
    "With this project we'll attempt to classify if an individual has heart disease using the Heart Disease Dataset from UCI. This dataset originates from the Clevland Clinic, and it's columns are broken down below: \n",
    "\n",
    "`id`: Unique id for each patient\n",
    "\n",
    "`age`: Age of the patient in years\n",
    "\n",
    "`sex`: Male/Female\n",
    "\n",
    "`cp`: chest pain type \n",
    "* 1 = typical angina \n",
    "* 2 = atypical angina \n",
    "* 3 = non-anginal \n",
    "* 4 = asymptomatic\n",
    "\n",
    "`trestbps`: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "\n",
    "`chol`: serum cholesterol in mg/dl\n",
    "\n",
    "`fbs`: if fasting blood sugar > 120 mg/dl\n",
    "\n",
    "`restecg`: resting electrocardiographic results \n",
    "* 0 = normal\n",
    "* 1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "* 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "\n",
    "`thalach`: maximum heart rate achieved\n",
    "\n",
    "`exang`: exercise-induced angina (True/ False)\n",
    "\n",
    "`oldpeak`: ST depression induced by exercise relative to rest\n",
    "\n",
    "`slope`: the slope of the peak exercise ST segment \n",
    "* 1 = upsloping \n",
    "* 2 = flat\n",
    "* 3 = downsloping\n",
    "\n",
    "`ca`: number of major vessels (0-3) colored by fluoroscopy\n",
    "\n",
    "`thal`: Thallium Stress Test\n",
    "* 3 = normal \n",
    "* 6 = fixed defect\n",
    "* 7 = reversible defect\n",
    "\n",
    "`present`: diagnosis of heart disease (the predicted attribute) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heart_disease.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#import dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m hd \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheart_disease.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'heart_disease.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#import dataset\n",
    "hd = pd.read_csv('heart_disease.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the column descripton, it should be obvious that some of these features need to be converted into dummies if they are to be used in our model. Namely those columns which represent categorical variables: `cp`, `restecg`, `slope`, and `thal`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd[['ca','thal']].value_counts()\n",
    "#there are some question marks turning these columns into object dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix ?'s -- just going to replace them with 0.0 and 3.0 respectively \n",
    "\n",
    "hd['ca'] = hd['ca'].replace('?', 0.0)\n",
    "hd['thal'] = hd['thal'].replace('?', 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd[['ca','thal']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert cols to int \n",
    "\n",
    "hd['ca'] = hd['ca'].astype(float).astype(int)\n",
    "hd['thal'] = hd['thal'].astype(float).astype(int)\n",
    "\n",
    "hd[['ca','thal']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert the relevant categorical columns into dummy variables for use in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = ['cp', 'restecg', 'slope', 'thal']\n",
    "\n",
    "hd = pd.get_dummies(hd, columns=cate)\n",
    "\n",
    "hd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to drop one of each to satisfy multicollinearity assump \n",
    "\n",
    "hd.drop(['cp_4', 'restecg_2', 'slope_3', 'thal_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data appears to be ready to move on to the next stage... feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We are building a single-class classification model based on the `present` column. Let's have a look at some of the potential predictors, and how they relate to our chosen outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at outcome \n",
    "\n",
    "hd['present'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts for cases and non-cases are similar. \n",
    "\n",
    "Because we don't have many features, we are going to test them all and see which combination works best. To do so we will use Recursive Feature Elimination (RFE), that trains a model on a subset of the features and recursivley considers increasingly smaller sets of features. \n",
    "\n",
    "The above method is a more economical means of selecting features given the presence of both categorical (ordinal and non-ordinal), and quantitative predictors in our data. If instead we wished to take a filter approach, we'd have to calculate multiple statistical tests given the nature of the features \n",
    "\n",
    "Example statistical tests for comparing variables of different kind: \n",
    "\n",
    "* ANOVA: one categorical variable one continuous variable\n",
    "* CHI-Squared: two categorical variables\n",
    "* Pearsons-r: two continuous variables (works with categorical variables that are numerical and ordinal)\n",
    "\n",
    "See: https://medium.com/@rithpansanga/logistic-regression-for-feature-selection-selecting-the-right-features-for-your-model-410ca093c5e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building The Model\n",
    "\n",
    "It's now time to construct our model. Before we do, it's worth noting that we are not going to proceed by scaling out data. Like with linear regression, feature scaling is not strictly required as neither algorithm relies on distance (unlike models such as KNN). However, when employing optimization algorithms like gradient descent, feature scaling can increase the time to convergence of the model, ultimatley reducing compute. \n",
    "\n",
    "See:\n",
    "\n",
    "https://forecastegy.com/posts/does-logistic-regression-require-feature-scaling/#:~:text=To%20put%20it%20simply%2C%20feature,applied%20uniformly%20across%20all%20features.\n",
    "\n",
    "https://forecastegy.com/posts/does-linear-regression-require-feature-scaling/#:~:text=In%20linear%20regression%2C%20feature%20scaling,convergence%20and%20improve%20model%20performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "\n",
    "X = hd.drop('present', axis=1)\n",
    "y = hd['present']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=734)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check y_train and y_test for cases and non cases\n",
    "\n",
    "print('y_train split:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('y_test split:')\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "#looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#building model \n",
    "model = LogisticRegression()\n",
    "\n",
    "#instantiating object of class RFE so we can feature select \n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "#fitting to training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "#getting top features\n",
    "top_feats = rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking top features\n",
    "final_feats = X_train.columns[top_feats].values\n",
    "\n",
    "#printing\n",
    "print(f'Best Features:') \n",
    "print(*final_feats, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RFE, the 5 most worthwhile features are:\n",
    "\n",
    "`exang`, `ca`, `cp_1`, `slope_2`, `thal_7`\n",
    "\n",
    "Lets build a model and calculate some performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing model\n",
    "final_model = LogisticRegression()\n",
    "\n",
    "#constraining training and testing data\n",
    "X_train_final = X_train[final_feats]\n",
    "X_test_final = X_test[final_feats]\n",
    "\n",
    "#fitting to training data\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "#getting predictions\n",
    "y_pred_train = final_model.predict(X_train_final)\n",
    "\n",
    "#grabbing metrics\n",
    "\n",
    "train_accuracy = final_model.score(X_train_final, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy.round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#creating confusion matrix for training data \n",
    "cm_train = confusion_matrix(y_train, y_pred_train, labels=final_model.classes_) #matrix\n",
    "\n",
    "cm_train_vis = ConfusionMatrixDisplay(cm_train, display_labels=final_model.classes_) #matrix viz\n",
    "cm_train_vis.plot()\n",
    "\n",
    "#calculating sensitivity and specificity from cm_train\n",
    "\n",
    "tp = cm_train[1,1]\n",
    "fn = cm_train[1,0]\n",
    "tn = cm_train[0,0]\n",
    "fp = cm_train[0,1]\n",
    "\n",
    "sens_train = tp / (tp+fn)\n",
    "print(f'Sensitivity: {sens_train.round(4)}')\n",
    "\n",
    "spec_train = tn / (tn+fp)\n",
    "print(f'Specificity: {spec_train.round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Discussion on Binary Classification Metrics\n",
    "\n",
    "<br>\n",
    "\n",
    "Below is a quick run-down of the performance metrics calculated from the training set: \n",
    "\n",
    "\n",
    "**Accuracy**: This is the percentage of the total observations that were correctly predicted\n",
    "\n",
    "**Sensitivity / Recall**: This is the percentage of cases (class = 1) that were correctly predicted\n",
    "\n",
    "$$\n",
    "\\frac{\\text{# correctly predicted cases}}{\\text{# total cases}} = \n",
    "\\frac{\\text{# correctly predicted subjects with heart disease}}{\\text{# subjects in dataset with heart disease}}\n",
    "$$\n",
    "\n",
    "**Specificity**: This is the percentage of non-cases (class = 0) that were correctly predicted\n",
    "\n",
    "$$\n",
    "\\frac{\\text{# correctly predicted non-cases}}{\\text{# total non-cases}} =\n",
    "\\frac{\\text{# correctly predicted subjects without heart disease}}{\\text{# subjects in dataset without heart disease}}\n",
    "$$\n",
    "\n",
    "We can also aknowledge another metric that, for simplicity's sake, we are not calculating, but is still important:\n",
    "\n",
    "**Precision**: This is the number of predicted cases that were correct\n",
    "\n",
    "$$\n",
    "\\frac{\\text{# correctly predicted cases}}{\\text{# total predictions made for cases}} = \n",
    "\\frac{\\text{# correctly predicted subjects with heart disease}}{\\text{# subjects predicted to have heart disease}}\n",
    "$$\n",
    "\n",
    "### Training Evaluation\n",
    "\n",
    "This model appears to perform better for non-cases than it does cases. This is not preferable as it means the model is worse at limiting the amount of false negatives than it is false positives. False negatives represent a situation where a person has been told they don't have heart disease when, in fact, they do. \n",
    "\n",
    "However, it is important to keep in mind that these results are based on our training data, which is overly optimistic. Let's move on to checking the model coefficients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get coefs\n",
    "coefs = final_model.coef_\n",
    "\n",
    "#get log odds ratio\n",
    "\n",
    "print('Log Odds-Ratio:')\n",
    "for i in range(5):\n",
    "    \n",
    "    coef = coefs[0][i]\n",
    "    name = final_feats[i]\n",
    "    \n",
    "    \n",
    "    print(name, ':', coef.round(4))\n",
    "\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "#get odds ratio \n",
    "\n",
    "print('Odds-Ratio:')\n",
    "for i in range(5):\n",
    "    \n",
    "    coef = coefs[0][i]\n",
    "    name = final_feats[i]\n",
    "    \n",
    "    odds_rat = np.exp(coef)\n",
    "    \n",
    "    print(name, ':', odds_rat.round(4))\n",
    "    \n",
    "my_label = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Looking at the above coefficients it's important to keep in mind that they represent the log odds-ratio for a unit increase in the given predictor, holding all other predictors constant. Consider the below example for the `ca` feature: \n",
    "\n",
    "> For a 1 unit change in `ca`, the log odds ratio changes by 1.02, holding all other predictors constant.  \n",
    "\n",
    "<br>\n",
    "\n",
    "To get the odds ratio, we raise e to each coefficient. First, recall that the odds ratio represents the following:\n",
    "\n",
    "\n",
    "1. let $O_0$ be the odds that our outcome is a non-case (i.e. class = 0) \n",
    "\n",
    "2. let $O_1$ be the odds that our coutcome is a case (i.e. class = 1) \n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that odds represent the ratio of a successful outcome, $EY$, to an unsuccessful outcome:\n",
    "\n",
    "$$\\frac{EY}{1-EY}$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "The odds ratio is simply the fraction of the two odds: \n",
    "\n",
    "$$\n",
    "\\frac{O_0}{O_1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider two instances: \n",
    "\n",
    "1. The odds ratio is > 1\n",
    "- this means that the odds that the outcome is a case (class = 1) are greater than 1 \n",
    "       \n",
    "2. The odds ratio is < 1 \n",
    "- this means that the odds that the outcome is a non-case (class = 0) are greater than 1\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's now consider the odds ratio for discrete-numerical feature `ca`, or, more correctly, ${e}^{\\beta_{ca}}$:\n",
    "\n",
    "> Changing the predictor by 1 unit changed the odds ratio by ${e}^{\\beta_{ca}}$, or, 2.79, holding all other predictors constant. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "For a predictor represented by a dummy variable, like `cp_1`, the coefficient takes on a different meaning. Generally speaking, a dummy coefficient represents the odds ratio associated with the given dummy as compared to the control condition (i.e. the omitted dummy). \n",
    "\n",
    "> In the case of `cp_1`, the odds-ratio of .40 indicates the factor of the odds of having heart disease in category `cp_1` v.s. the odds of having heart disease within the reference category, `cp_4`. In other words, the odds ratio of .40 for category `cp_1`, means that having heart disease is less likely, .40 times as high, for this group than the refrence group. \n",
    "\n",
    "`cp` denotes chest pain type, `cp_1` represents one category from the 4 possible levels recorded by the researchers. \n",
    "\n",
    "`cp_1` : typical angina \n",
    "\n",
    "`cp_4` : asymptomatic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Before we finally evaluate the model, for the sake of completeness, it would be beneficial for us to aknowledge that, in the case of a formal analysis, an assumption check is required to establish the applicability of the underlying model to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "It's now time to evaluate our model on the testing data. We have a general idea of how the model performs on the training data, but those results are not reflective of reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "\n",
    "#evaluate predictions\n",
    "\n",
    "test_accuracy = final_model.score(X_test_final, y_test)\n",
    "print(f'Accuracy: {test_accuracy.round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating confusion matrix for testing data \n",
    "cm_test = confusion_matrix(y_test, y_pred_test, labels=final_model.classes_) #matrix\n",
    "\n",
    "cm_train_vis = ConfusionMatrixDisplay(cm_test, display_labels=final_model.classes_) #matrix viz\n",
    "cm_train_vis.plot()\n",
    "\n",
    "#calculating sensitivity and specificity from cm_test\n",
    "\n",
    "tp = cm_test[1,1]\n",
    "fn = cm_test[1,0]\n",
    "tn = cm_test[0,0]\n",
    "fp = cm_test[0,1]\n",
    "\n",
    "sens_test = tp / (tp+fn)\n",
    "print(f'Sensitivity: {sens_test.round(4)}')\n",
    "\n",
    "spec_test = tn / (tn+fp)\n",
    "print(f'Specificity: {spec_test.round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and sensitivity are both lower for the testing data as opposed to the training data. However, specificity returned a higher value after being evaluated on the testing data. Consequently, the model's superior predictive performance for non-cases is more evident after being evaluated on the testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Post our final evaluation on the training data, it's now time to consider the utility of this model.\n",
    "\n",
    "Being that the model predicts non-cases better, alot better, than cases, this model has a glaring problem in the form of false negatives. While, after evaluation on the testing data, 90% of non-cases were correctly identified, the same could be said of only 72% of cases. Contextually, this means that not enough people with heart disease are being correctly identified. In other words, there exist too many individuals for which the model states they do not have heart disease when, in actuality, they do. From a clinical perspective, it's pretty obvious to see why this presents a problem -- the affected group is not going to be prioritized for treatment. \n",
    "\n",
    "As such, the fairly decent accuracy figure post-evaluation on the testing data does not tell the whole story. \n",
    "\n",
    "To fix this model we could try a few things: \n",
    "* getting more data\n",
    "* hyperparameter tuning\n",
    "* choosing different features (or perhaps a different method for selecting features)\n",
    "* splitting the training and testing sets up differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
